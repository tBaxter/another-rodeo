---
title: "AI is boring and stupid and maybe that's OK"
description: "Sometimes boring and stupid is still really useful."
date: 2024-03-07
eleventyExcludeFromCollections: true
parent: 'Writing'
img_path: /img/writing/ai/
thumb: assembly.jpeg
#tags:
#    - writing
#    - post
#    - ai
#    - engineering
---

  {% figure img_path thumb "Attempting to spot defects on a fast-moving assembly line." %}

We’re now just a bit over two years into the AI revolution. The chorus of hype and promises — both empty and real — remains deafening. The capabilities of AI increase every day. Yet right now, the most impressive thing about AI may be how quickly we’ve trained our primitive monkey brains to recognize how incredibly boring it is.

In months, really, we’ve retrained our brains. We've gone from being utterly dazzled by the magic of AI to seeing the machine behind the curtain, moving the levers. Not long ago we watched incredulously as AI conjured credible art out of thin air, held lengthy conversations with us on the fly, and generated new lyrics for Taylor Swift songs anytime we liked. After a moment of magic, we started to shrug.

Because in the end, AI as a capability may be fascinating, but the output of it is nearly always dull and plodding. Predictable. Recognizable as the work of a machine. The AI-generated art proliferating across blog posts and the pages of LinkedIn Thought Leaders has already become a cliche. When we see someone pass off AI-generated text as their own, we know. 

We _know_.

We’ve learned to see it, and just as we can’t help but see the green screen effect in an old sci-fi movie, we’ll only get better at spotting it, until eventually it's all we can see. 

In the case of AI, we recognize it for what it is: Boring. Lifeless. Trite. Been there, done that, got the AI-generated T-shirt.

  {% figure img_path 'ai-t-shirt.jpeg' "Inspiring, isn't it?" %}

AI is built to be boring. That’s literally how it works: it predicts the most likely next word or pixel, based on the gazillion words and pixels fed to it. The problem is you, you’ve likely seen some version of the words and pixels fed to it before. Not every permutation, but enough to recognize the lack of originality. 

And so it goes with the most visible use cases for AI today: We see AI-generated art and reject both the processed sameness of it as quickly as we recoil from the six-fingered uncanny valley. 

We gloss over AI-generated text the same as we’ve learned to gloss over text that’s clearly overly optimized for SEO. Whether written by machines or for machines, our human brains want text written for humans, quirks and non-sequiturs and all. We want humor and humanity and something that makes us think in a way we haven’t thought before. 

Creativity doesn’t exist without unpredictability. Good doesn’t exist with humanity. Not yet at least.

So, in a remarkably short time we’ve trained ourselves to see the machines working, and we recognize their work is largely... boring. And yet, this is not a problem for AI. Or for people. Because a lot of what we do every day really is boring. Repetitive. Predictable. The sort of thing the machines literally trained on.

<div style="position: relative; width: 100%; padding-bottom: 56.25%;">
<iframe
style="position:absolute; width:100%; height:100%;"
src="https://getyarn.io/yarn-clip/474d7a6b-f993-4713-8a7b-60a778ec1454/embed?autoplay=false&responsive=true"
frameborder="0"
></iframe>
</div>

### Boring can be OK

The world is full of things that exist because they need to exist to fulfill some requirement, not because they need to bring us joy. Sometimes we just need to get something done in order to get to something more important to us. 

We write an awful lot of boilerplate and scaffolding on our way to something else, and then we write a lot of other things just to check a box. Boilerplate code. Boilerplate legal documents. Boilerplate grants and guidance and RFPs and SOWs and a whole alphabet soup of other things. 

We don’t need these things to be good, we need them to be done. This is where AI can really shine. It’s great at creating all that boilerplate and scaffolding, and then it can just as easily turn around and consume it all and spit out a nice, concise summary. 

If we really over-achieve we can build a future where a lot of content isn’t written or read by humans much at all. And maybe that’s not a bad thing. I mean, a lot of this stuff was never all that human-centered or human-readable to begin with.

The worrisome part is that nobody reads this stuff now, and any editor (or coder) knows that it’s easy to gloss over errors when something is already written. 

### So what if errors sneak in? AI can catch those, too.

Sure. Maybe. But will they catch them _before_ real people are impacted? I don't think so. We know real people are impacted already, starting with the people who created the works AI is regurgitating. But as more and more AI-generated text creeps into more places – especially highly regulated places – what will it bring along for the ride?

This is especially worrisome in my field – government technology – where what's written determines which people receive needed benefits. Which Veterans get services. Which mothers are able to feed their children today. Which small businesses will survive thanks to a government loan.

Literally, an entire world hinges simply on what's written and how it's read. And this is, truly, exactly the sort of text that AI excels at generating.

My very smart friend Mark Headd recently wrote up [some pretty smart use cases for AI and LLMs in government](https://adhoc.team/2024/02/27/LLMs-gov-future/). Bill Hunt, another very smart friend also working in government, has [a pretty strong opinion that today’s AI models aren’t ready](https://billhunt.dev/blog/2023/10/09/llms-are-not-government-ready/). I think it’s possible that they’re both right. 

This is happening now in highly regulated places. Government systems are using AI to detect fraud and waste, to help agencies with regulatory compliance and, undoubtedly, a lot of other things. With a lot of questions about what happens when there are false positives, or false negatives. And that's today. The most touted use cases remain largely on the horizon. 

Before we reach that horizon we’re going to have to figure out how to handle editorial control, quality checking, rules compliance and a host of other factors for anything AI-generated. 

We’re going to have to find a way to ensure that AI is producing something like what human intelligence would have produced and that it is not introducing a bunch of errors. Getting AI to generate boilerplate is the easy part. Figuring out how we ensure that it’s correct will be the bear we have to wrestle.

This is not the first time we've had this problem.

### The assembly line
True fact: The precursor to modern assembly lines – arguably the first assembly lines – could be found in 19th century meatpacking plants. There you'd find overhead trolleys moving heavy carcasses from worker to worker for processing. At the time "processing" generally meant large knives and saw, so conditions were dire . Defects and accidents were largely ignored -- even though they could be deadly.

When automotive assembly lines in the early 20th century spurred a manufacturing revolution across industries, defects and errors still abounded, especially when plant owners made the the machines run faster to drive greater productivity. They got the idea from English textile companies, who had done for years with their looms -- much to the horror of Luddites, who really just wanted a living wage and respect for workers. But we were talking about errors....

It wasn't until well into the 20th century that manufacturers really began to master multi-stage, rigorous quality control. Even then, quality in many industries has remained spotty and sporadic. 

And that was when inspectors and testers only had to move at the speed of the line, which was limited at each step by what one person could do. Today we've automated the creation of nonsense at mass scale and still need to figure out how we'll insert quality checks.

We already know that [AI-generated code lowers quality](https://visualstudiomagazine.com/Articles/2024/01/25/copilot-research.aspx). We can already clearly see that it will do the same with generated text, and that can be a real problem when compliance, correctness and contractual obligations are on the line. The question is, what are we going to do about it?

I just don’t think we can trust humans to be the answer. Even if they want to, I don't think they'll be able to keep up with fact-checking and error-checking all the content AI can generate. And we can't count on AI to do it, because that generates another layer of AI we'd need to check. It's a hall of mirrors.

So, at least for now, we find ourselves hacking through an ever-deepening forest of generated content, hoping we can spot the bear lurking in the forest before it mauls us. 

We're not going to be good at this. That forest of AI-generated content is boring as hell. And yet, somehow, we'll find a path through it. It will be OK. 

So long as we don't use up all the electricity in the world first. In the meantime, here's a picture of Joe Biden and Donald Trump enjoying some quality time together.

  {% figure img_path 'biden-trump.jpg' "Reality is the only word in the English language that should always be used in quotes." %}
